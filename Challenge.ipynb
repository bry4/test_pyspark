{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgNUHvQ9XGe770aus1dYr1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bry4/test_pyspark/blob/main/Challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TESTEADO EN GOOGLE COLAB\n",
        "# Estructura de archivos en capas en base a proyectos en Databricks\n",
        "!rm -r sample_data/\n",
        "!mkdir 00_raw\n",
        "!mkdir 01_bronze\n",
        "!mkdir 02_silver\n",
        "!mkdir 03_gold"
      ],
      "metadata": {
        "id": "YE9pac_O5xHS"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PARA AMBIENTES DE PRUEBA O DESARROLLO - Testeado en Google Colab\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPtAHvx-3rAD",
        "outputId": "1097895e-8341-4800-a831-e8b98ccdab56"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=d7fb4f8e1322964a4360851fcb261dc425b44d2796b751034c449788aee2d226\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/dc/11/ec201cd671da62fa9c5cc77078235e40722170ceba231d7598\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importando librerías a considerar\n",
        "# Armando Clase con funciones útiles para desarrollar el ETL\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "class sparkChallenge:\n",
        "\n",
        "  def __init__(self):\n",
        "    spark = SparkSession.builder.master(\"local[*]\").appName(\"appChallenge\").getOrCreate()\n",
        "    self._spark = spark\n",
        "  \n",
        "  def get_session(self):\n",
        "    return self._spark\n",
        "\n",
        "  def read_csv_rdd(self, path:str):\n",
        "\n",
        "    rdd = self._spark.sparkContext.textFile(path)\n",
        "    return rdd\n",
        "  \n",
        "  def read_csv_df(self, path:str):\n",
        "\n",
        "    df = self._spark.read.format(\"csv\").option(\"header\",\"true\").load(path)\n",
        "    return df\n",
        "\n",
        "  def read_parquet(self, path:str):\n",
        "    \n",
        "    df = spark.read.parquet(path)\n",
        "    return df\n",
        "\n",
        "  def write_parquet(self, df_input:DataFrame, path:str):\n",
        "    \n",
        "    df_input.write.mode(\"overwrite\").parquet(path)\n",
        "\n",
        "\n",
        "  def clean_column_names(self,df_input:DataFrame)->DataFrame:\n",
        "\n",
        "    # Limpieza de nombres de columnas por cada caso encontrado (se puede usar regex pero se agregarían más líneas de código)\n",
        "    for x in df_input.schema.names:\n",
        "      df_input = df_input.withColumnRenamed(x,x.lower())\n",
        "\n",
        "    for x in df_input.schema.names:\n",
        "      df_input = df_input.withColumnRenamed(x,x.replace(\" / \",\"_\"))\n",
        "\n",
        "    for x in df_input.schema.names:\n",
        "      df_input = df_input.withColumnRenamed(x,x.replace(\" \",\"_\"))\n",
        "\n",
        "    for x in df_input.schema.names:\n",
        "      df_input = df_input.withColumnRenamed(x,x.replace(\".\",\"\"))\n",
        "\n",
        "    for x in df_input.schema.names:\n",
        "      df_input = df_input.withColumnRenamed(x,x.replace(\"/\",\"_\"))\n",
        "    \n",
        "    return df_input\n",
        "\n",
        "  def find_null_columns(self,df_input:DataFrame, txt_input:str)->list:\n",
        "    \n",
        "    list_col = []\n",
        "    for x in df_input.schema.names:\n",
        "      count_null = df_input.filter(~col(x).isNotNull()).count() # Contar nulos por cada columna recorrida del dataframe\n",
        "\n",
        "      if count_null>0:\n",
        "        list_col.append(x) # Listando columnas con valores nulos por cada Dataframe\n",
        "\n",
        "    if len(list_col)==0:\n",
        "      print(\"No hay columnas con valores nulos en: \"+txt_input)\n",
        "    \n",
        "    return list_col\n",
        "\n",
        "  def replace_null_values(self,df_input:DataFrame, list_col:list)->DataFrame:\n",
        "\n",
        "    if len(list_col)>0:\n",
        "      \n",
        "      for x in list_col:\n",
        "        isnum_df = df_input.filter(df_input[x].rlike('\\D+')).count() # Contar la existencia de valores String\n",
        "\n",
        "        if isnum_df>0: # Reemplazar \"Otros\" a columnas con una cantidad mayor a Cero de valores String (Columnas con tipo dato String)\n",
        "          df_input = df_input.withColumn(x,when(df_input[x].isNull(),\"Otros\").otherwise(df_input[x]))\n",
        "        \n",
        "        if isnum_df==0: # Reemplazar 0 a columnas con una cantidad igual a Cero de valores String (Columnas con tipo dato Numerico)\n",
        "          df_input = df_input.withColumn(x,when(df_input[x].isNull(),0).otherwise(df_input[x]))\n",
        "\n",
        "    return df_input\n",
        "\n",
        "  def transform_type(self,df_input:DataFrame, typeTF:str, listVal:list)->DataFrame: # Dar Formato de tipo de datos a tablas según lista de campos o tipo de dato\n",
        "    if typeTF == \"integer\":\n",
        "      for x in listVal:\n",
        "        df_input = df_input.withColumn(x,col(x).cast(\"integer\"))\n",
        "\n",
        "    if typeTF == \"date\":\n",
        "      for x in listVal:\n",
        "        df_input = df_input.withColumn(x,to_date(df_input[x]))\n",
        "\n",
        "    if typeTF == 'decimal':\n",
        "      for x in listVal:        \n",
        "        df_input = df_input.withColumn(x,regexp_replace(col(x),' ','').cast('decimal(22,3)'))\n",
        "              \n",
        "    if typeTF == 'double':\n",
        "      for x in listVal:\n",
        "        df_input = df_input.withColumn(x,regexp_replace(col(x),' ','').cast('double'))\n",
        "\n",
        "    return df_input\n",
        "\n"
      ],
      "metadata": {
        "id": "8oAAKFt84IlC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instanciando Clase\n",
        "SC = sparkChallenge()\n",
        "spark = SC.get_session() # Obteniendo sesion para uso de propiedades particulares de Spark Dataframe"
      ],
      "metadata": {
        "id": "A6IpfWiZQzTO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargando dataset en RDD\n",
        "rdd_country = SC.read_csv_rdd(\"/content/00_raw/country_wise_latest.csv\")\n",
        "rdd_covid_clean =  SC.read_csv_rdd(\"/content/00_raw/covid_19_clean_complete.csv\")\n",
        "rdd_day =  SC.read_csv_rdd(\"/content/00_raw/day_wise.csv\")\n",
        "rdd_full_grouped =  SC.read_csv_rdd(\"/content/00_raw/full_grouped.csv\")\n",
        "rdd_usa_county =  SC.read_csv_rdd(\"/content/00_raw/usa_county_wise.csv\")\n",
        "rdd_worldmeter =  SC.read_csv_rdd(\"/content/00_raw/worldometer_data.csv\")"
      ],
      "metadata": {
        "id": "bomUMSB8SRPb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculando cantidad de registros por archivo fuente\n",
        "rdd_count = [{\n",
        "    \"country\":rdd_country.count()-1,\n",
        "    \"covid_clean\":rdd_covid_clean.count()-1,\n",
        "    \"day\":rdd_day.count()-1,\n",
        "    \"full_grouped\":rdd_full_grouped.count()-1,\n",
        "    \"usa_county\":rdd_usa_county.count()-1,\n",
        "    \"worldmeter\":rdd_worldmeter.count()-1\n",
        "}]\n",
        "df_rdd_count = spark.createDataFrame(rdd_count)\n",
        "\n",
        "SC.write_parquet(df_rdd_count,\"/content/01_bronze/rowcount_files.parquet\")"
      ],
      "metadata": {
        "id": "yfHiNyQtWAbM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargando dataset en Dataframe\n",
        "df_country = SC.read_csv_df(\"/content/00_raw/country_wise_latest.csv\")\n",
        "df_covid_clean = SC.read_csv_df(\"/content/00_raw/covid_19_clean_complete.csv\")\n",
        "df_day = SC.read_csv_df(\"/content/00_raw/day_wise.csv\")\n",
        "df_full_grouped = SC.read_csv_df(\"/content/00_raw/full_grouped.csv\")\n",
        "df_usa_county = SC.read_csv_df(\"/content/00_raw/usa_county_wise.csv\")\n",
        "df_worldmeter = SC.read_csv_df(\"/content/00_raw/worldometer_data.csv\")\n",
        "\n",
        "# Cargando en formato parquet\n",
        "SC.write_parquet(df_country,\"/content/01_bronze/country_wise_latest.parquet\")\n",
        "SC.write_parquet(df_covid_clean,\"/content/01_bronze/covid_19_clean_complete.parquet\")\n",
        "SC.write_parquet(df_day,\"/content/01_bronze/day_wise.parquet\")\n",
        "SC.write_parquet(df_full_grouped,\"/content/01_bronze/full_grouped.parquet\")\n",
        "SC.write_parquet(df_usa_county,\"/content/01_bronze/usa_county_wise.parquet\")\n",
        "SC.write_parquet(df_worldmeter,\"/content/01_bronze/worldometer_data.parquet\")\n"
      ],
      "metadata": {
        "id": "qdRVOgrh84bI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Limpiando nombre de columnas\n",
        "df_country = SC.clean_column_names(df_country)\n",
        "df_covid_clean = SC.clean_column_names(df_covid_clean)\n",
        "df_day = SC.clean_column_names(df_day)\n",
        "df_full_grouped = SC.clean_column_names(df_full_grouped)\n",
        "df_usa_county = SC.clean_column_names(df_usa_county)\n",
        "df_worldmeter = SC.clean_column_names(df_worldmeter)"
      ],
      "metadata": {
        "id": "or7Lygt3tbIx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Listando columnas nulas para tratarlos más adelante\n",
        "list_country = SC.find_null_columns(df_country, \"country\")\n",
        "list_covid_clean = SC.find_null_columns(df_covid_clean, \"covid_clean\")\n",
        "list_day = SC.find_null_columns(df_day, \"day\")\n",
        "list_full_grouped = SC.find_null_columns(df_full_grouped, \"full_grouped\")\n",
        "list_usa_county = SC.find_null_columns(df_usa_county, \"usa_county\")\n",
        "list_worldmeter = SC.find_null_columns(df_worldmeter, \"worldmeter\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhFVSXkm0Tff",
        "outputId": "664cac47-9605-4837-e8cd-16b101f0bb77"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No hay columnas con valores nulos en: country\n",
            "No hay columnas con valores nulos en: day\n",
            "No hay columnas con valores nulos en: full_grouped\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reemplazando nulos ya sean campos String o Numericos\n",
        "df_country = SC.replace_null_values(df_country, list_country)\n",
        "df_covid_clean = SC.replace_null_values(df_covid_clean, list_covid_clean)\n",
        "df_day = SC.replace_null_values(df_day, list_day)\n",
        "df_full_grouped = SC.replace_null_values(df_full_grouped, list_full_grouped)\n",
        "df_usa_county = SC.replace_null_values(df_usa_county, list_usa_county)\n",
        "df_worldmeter = SC.replace_null_values(df_worldmeter, list_worldmeter)"
      ],
      "metadata": {
        "id": "ktfBH_2_0iOg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CAPA SILVER\n",
        "# Dandole formato a las tablas para el modelo en la capa Silver\n",
        "# Se ha modelado en base a contener la mayor cantidad de columnas por entidad aprovechando el uso de parquet\n",
        "\n",
        "df_covid_clean = df_covid_clean.select(concat(df_covid_clean.country_region,df_covid_clean.province_state). \\\n",
        "                      alias(\"id_province\"),\"country_region\",\"province_state\",\"lat\",\"long\",\"date\",\"confirmed\",\"deaths\",\"recovered\",\"active\")\n",
        "\n",
        "group_col = [\"id_province\",\"country_region\",\"province_state\",\"lat\",\"long\"]\n",
        "# Tabla province_covid\n",
        "df_province_covid = df_covid_clean.groupBy(group_col). \\\n",
        "                                  agg(sum(\"confirmed\").alias(\"confirmed\"), \\\n",
        "                                      sum(\"deaths\").alias(\"deaths\"), \\\n",
        "                                      sum(\"recovered\").alias(\"recovered\"), \\\n",
        "                                      sum(\"active\").alias(\"active\"))\n",
        "# Dando formato de tipo datos\n",
        "list1_double = [\"lat\",\"long\"]\n",
        "list1_int = [\"confirmed\",\"deaths\",\"recovered\",\"active\"]\n",
        "df_province_covid = SC.transform_type(df_province_covid,\"double\",list1_double)\n",
        "\n",
        "SC.write_parquet(df_province_covid,\"/content/02_silver/province_covid.parquet\")\n",
        "\n",
        "# Tabla daily_province_covid\n",
        "df_daily_province_covid = df_covid_clean[\"id_province\",\"date\",\"confirmed\",\"deaths\",\"recovered\",\"active\"]\n",
        "\n",
        "list2_time = [\"date\"]\n",
        "list2_double = [\"confirmed\",\"deaths\",\"recovered\",\"active\"]\n",
        "df_daily_province_covid = SC.transform_type(df_daily_province_covid,\"date\",list2_time)\n",
        "df_daily_province_covid = SC.transform_type(df_daily_province_covid,\"double\",list2_double)\n",
        "\n",
        "SC.write_parquet(df_daily_province_covid,\"/content/02_silver/daily_province_covid.parquet\")\n",
        "\n",
        "# Tabla daily_covid\n",
        "df_daily_covid = df_day[\"date\",\"confirmed\",\"deaths\",\"recovered\",\"active\"]\n",
        "\n",
        "list3_time = [\"date\"]\n",
        "list3_double = [\"confirmed\",\"deaths\",\"recovered\",\"active\"]\n",
        "df_daily_covid = SC.transform_type(df_daily_covid,\"date\",list3_time)\n",
        "df_daily_covid = SC.transform_type(df_daily_covid,\"double\",list3_double)\n",
        "\n",
        "SC.write_parquet(df_daily_covid,\"/content/02_silver/daily_covid.parquet\")\n",
        "\n",
        "# Tabla country\n",
        "df_country_world = df_worldmeter[\"who_region\",\"continent\",\"country_region\",\"population\"]\n",
        "\n",
        "list4_int = [\"population\"]\n",
        "df_country_world = SC.transform_type(df_country_world,\"integer\",list4_int)\n",
        "\n",
        "SC.write_parquet(df_country_world,\"/content/02_silver/country.parquet\")\n",
        "\n"
      ],
      "metadata": {
        "id": "hjwi7TtJ-HwT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CAPA GOLD\n",
        "# Para el caso de reportería tendría que ser un modelo más simple (ESTRELLA, TABLÓN) para su efectivo consumo\n",
        "# En este caso sacamos todos los datos unidos en una tabla a nivel Fecha\n",
        "\n",
        "df_province_covid = SC.read_parquet(\"/content/02_silver/province_covid.parquet\")\n",
        "df_daily_province_covid = SC.read_parquet(\"/content/02_silver/daily_province_covid.parquet\")\n",
        "df_daily_covid = SC.read_parquet(\"/content/02_silver/daily_covid.parquet\")\n",
        "df_country = SC.read_parquet(\"/content/02_silver/country.parquet\")\n",
        "\n",
        "df_country = df_country.withColumnRenamed(\"country_region\",\"country_reg\")\n",
        "\n",
        "df_f1 = df_province_covid.join(df_country, df_province_covid.country_region==df_country.country_reg,\"left\")\n",
        "df_f2 = df_f1[\"id_province\",\"who_region\",\"continent\",\"country_region\",\"population\",\"province_state\",\"lat\",\"long\"]\n",
        "df_f2 = df_f2.withColumnRenamed(\"id_province\",\"id_prov\")\n",
        "\n",
        "df_final = df_daily_province_covid.join( \\\n",
        "    df_f2, \\\n",
        "    df_daily_province_covid.id_province==df_f2.id_prov,\"left\")\n",
        "\n",
        "df_final = df_final[\"id_province\",\"who_region\",\"continent\",\"country_region\",\"population\",\"province_state\",\"lat\",\"long\",\"date\",\"confirmed\",\"deaths\",\"recovered\",\"active\"]\n",
        "\n",
        "SC.write_parquet(df_final,\"/content/03_gold/report.parquet\")\n"
      ],
      "metadata": {
        "id": "3kC0yOj-jvgA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xl8wPlbvrjSq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}